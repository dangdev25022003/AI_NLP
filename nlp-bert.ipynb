{"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b11e0e4056a347c88c52243624c29b5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29e7247cf9da42ffa09939e0937be4f6","IPY_MODEL_1ba99172f5574d27a8bedffdfbd466e5","IPY_MODEL_ebd4c7ed379a46fbb1360bc81bb77294"],"layout":"IPY_MODEL_d47fe59d1a0040b4ab12f6a4d1952c15"}},"29e7247cf9da42ffa09939e0937be4f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cc29c5099ab4c83ab5c7866d961457c","placeholder":"​","style":"IPY_MODEL_9525b8c275d24b3e9ca724a47137721d","value":"Downloading (…)lve/main/config.json: 100%"}},"1ba99172f5574d27a8bedffdfbd466e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b5701ff1c8f48a385d7db9c57474227","max":678,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6757b19d9e844f6a8e5b8d0f8dc10f3b","value":678}},"ebd4c7ed379a46fbb1360bc81bb77294":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaeac0ee2c0c42c4904e061eb24312fc","placeholder":"​","style":"IPY_MODEL_8ad528247d4d4ecfb47065dd999d0bc7","value":" 678/678 [00:00&lt;00:00, 13.5kB/s]"}},"d47fe59d1a0040b4ab12f6a4d1952c15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cc29c5099ab4c83ab5c7866d961457c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9525b8c275d24b3e9ca724a47137721d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b5701ff1c8f48a385d7db9c57474227":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6757b19d9e844f6a8e5b8d0f8dc10f3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eaeac0ee2c0c42c4904e061eb24312fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ad528247d4d4ecfb47065dd999d0bc7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"555025445a034c44958ddb447a527543":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33d329ccb9ba430aae10c4dd0880f483","IPY_MODEL_724178cd49c54edd81c1a02f86a4f5fa","IPY_MODEL_b966d36c776d41e581d612e020e26b15"],"layout":"IPY_MODEL_356c37aa7da54a758e2569fbdc2777b8"}},"33d329ccb9ba430aae10c4dd0880f483":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b980379fdb94d8785c0600272f50bba","placeholder":"​","style":"IPY_MODEL_0cbf6a2f28c74c3e848377bbed0f4109","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"724178cd49c54edd81c1a02f86a4f5fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_80540b63342f4bab974f5fcdd786f0eb","max":895321,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd552cb9c9a749518b547439bf680f1b","value":895321}},"b966d36c776d41e581d612e020e26b15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4074b06fec946909a65780bf165605c","placeholder":"​","style":"IPY_MODEL_9acf94dca7f64b298dd120fd2b82a723","value":" 895k/895k [00:00&lt;00:00, 5.36MB/s]"}},"356c37aa7da54a758e2569fbdc2777b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b980379fdb94d8785c0600272f50bba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cbf6a2f28c74c3e848377bbed0f4109":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80540b63342f4bab974f5fcdd786f0eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd552cb9c9a749518b547439bf680f1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4074b06fec946909a65780bf165605c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9acf94dca7f64b298dd120fd2b82a723":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8ef79fd3c504b8280a388a5c1dd1095":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fd1e3876decd4975b09e0331524d184e","IPY_MODEL_bd7f69cdb6894c16bd90f65b4949a439","IPY_MODEL_6360ac2a395d45d08e70320f80766c80"],"layout":"IPY_MODEL_0601ec4b9677414d94de381db5f5ce55"}},"fd1e3876decd4975b09e0331524d184e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b63cfae0c72d425a93a0b114f436ef0a","placeholder":"​","style":"IPY_MODEL_7c0217fc772d4fb69b2452669f82c964","value":"Downloading (…)solve/main/bpe.codes: 100%"}},"bd7f69cdb6894c16bd90f65b4949a439":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5863d629b8794756bc640eb24774c721","max":1135173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddd5ecbd6f4b40179ed2b6313ba9baec","value":1135173}},"6360ac2a395d45d08e70320f80766c80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e68ba1554113468e8a4b22a7a9c381f9","placeholder":"​","style":"IPY_MODEL_e4999b05dc66400ebd999058210e8bde","value":" 1.14M/1.14M [00:00&lt;00:00, 13.5MB/s]"}},"0601ec4b9677414d94de381db5f5ce55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b63cfae0c72d425a93a0b114f436ef0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c0217fc772d4fb69b2452669f82c964":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5863d629b8794756bc640eb24774c721":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddd5ecbd6f4b40179ed2b6313ba9baec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e68ba1554113468e8a4b22a7a9c381f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4999b05dc66400ebd999058210e8bde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e898f3657c443ffadac7bc12e233ff8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9584375023814562babef20caf662c69","IPY_MODEL_5d456240a9344b99b82dcf4d4173c680","IPY_MODEL_da77928ac4464fcd95476f09d6452a3b"],"layout":"IPY_MODEL_35a71eec038f4f01a5c5c810cc534c32"}},"9584375023814562babef20caf662c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4663a30669d41c9aaee021b1acfb7eb","placeholder":"​","style":"IPY_MODEL_a678c53bd3a649c3bdaf3f36f4d6edb4","value":"Downloading (…)/main/tokenizer.json: 100%"}},"5d456240a9344b99b82dcf4d4173c680":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cfb3821863c4008af8732d25d6dc946","max":3132320,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7202f164dbca459089e8999e2ef9eadb","value":3132320}},"da77928ac4464fcd95476f09d6452a3b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54bce615b90945d2aa6d77634be90d73","placeholder":"​","style":"IPY_MODEL_1a420e50ed6b467183cd746f048ea7b5","value":" 3.13M/3.13M [00:00&lt;00:00, 30.7MB/s]"}},"35a71eec038f4f01a5c5c810cc534c32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4663a30669d41c9aaee021b1acfb7eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a678c53bd3a649c3bdaf3f36f4d6edb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3cfb3821863c4008af8732d25d6dc946":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7202f164dbca459089e8999e2ef9eadb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"54bce615b90945d2aa6d77634be90d73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a420e50ed6b467183cd746f048ea7b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c4c01c438c24bd4a6c8eb5a9b478713":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_08f72e1461fb417c86ff6fff0917e121","IPY_MODEL_09bc91778be446f1ba6a2ce35fc611ac","IPY_MODEL_4742cdad96514fafb7d469e4bfd67625"],"layout":"IPY_MODEL_6c2a6c2f76cb4252b99a3e9bab285f85"}},"08f72e1461fb417c86ff6fff0917e121":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dde6ea7e8a04e2b90f8e35fd4a11e61","placeholder":"​","style":"IPY_MODEL_27f92c78758743ab8357ddabfb830243","value":"Downloading pytorch_model.bin: 100%"}},"09bc91778be446f1ba6a2ce35fc611ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_70bac8180dc74d3d86766067c882d9e6","max":540322347,"min":0,"orientation":"horizontal","style":"IPY_MODEL_089b3c011d1e46fc80cc1d0c9aa7898b","value":540322347}},"4742cdad96514fafb7d469e4bfd67625":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb64ecd0056045efa1f0eaf0b241c987","placeholder":"​","style":"IPY_MODEL_3fde2f01c593450f8f7fe448ea198e64","value":" 540M/540M [00:04&lt;00:00, 143MB/s]"}},"6c2a6c2f76cb4252b99a3e9bab285f85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dde6ea7e8a04e2b90f8e35fd4a11e61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27f92c78758743ab8357ddabfb830243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"70bac8180dc74d3d86766067c882d9e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"089b3c011d1e46fc80cc1d0c9aa7898b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb64ecd0056045efa1f0eaf0b241c987":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fde2f01c593450f8f7fe448ea198e64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install underthesea"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8whyafiMUnfk","outputId":"0a469e6e-194a-4c8b-a339-a61fbf194e15","execution":{"iopub.status.busy":"2023-10-17T22:54:01.736301Z","iopub.execute_input":"2023-10-17T22:54:01.736633Z","iopub.status.idle":"2023-10-17T22:54:18.732586Z","shell.execute_reply.started":"2023-10-17T22:54:01.736611Z","shell.execute_reply":"2023-10-17T22:54:18.731637Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603240995,"user_tz":-420,"elapsed":29792,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n","Collecting underthesea\n","  Downloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n","Collecting python-crfsuite>=0.9.6 (from underthesea)\n","  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n","Collecting underthesea-core==1.0.4 (from underthesea)\n","  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.6.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.7.22)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.2.0)\n","Installing collected packages: underthesea-core, python-crfsuite, underthesea\n","Successfully installed python-crfsuite-0.9.9 underthesea-6.8.0 underthesea-core-1.0.4\n"]}]},{"cell_type":"code","source":["import os\n","import shutil\n","import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import transformers\n","from transformers import AutoModel, AutoTokenizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import underthesea as uts\n","import nltk\n","import torch\n","import torch.nn as nn\n","import numpy as np"],"metadata":{"id":"SN0eZirGUTxR","execution":{"iopub.status.busy":"2023-10-17T22:54:18.734357Z","iopub.execute_input":"2023-10-17T22:54:18.734623Z","iopub.status.idle":"2023-10-17T22:54:18.740365Z","shell.execute_reply.started":"2023-10-17T22:54:18.7346Z","shell.execute_reply":"2023-10-17T22:54:18.739657Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603249831,"user_tz":-420,"elapsed":8864,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["label1 = pd.read_csv(\"labeled_sentences_1.csv\", encoding='utf8', engine='python').dropna()\n","label2 = pd.read_csv(\"labeled_sentences_2.csv\", encoding='utf8', engine='python').dropna()\n","label3 = pd.read_csv(\"labeled_sentences_3.csv\", encoding='utf8', engine='python').dropna()\n","merge_data = pd.concat([label1, label2, label3]).reset_index(drop=True)"],"metadata":{"id":"O_QdzqEDVGYK","execution":{"iopub.status.busy":"2023-10-17T22:54:18.741211Z","iopub.execute_input":"2023-10-17T22:54:18.741496Z","iopub.status.idle":"2023-10-17T22:54:18.7931Z","shell.execute_reply.started":"2023-10-17T22:54:18.741474Z","shell.execute_reply":"2023-10-17T22:54:18.792395Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603251365,"user_tz":-420,"elapsed":1562,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["merge_data['Sentiment'] = merge_data['Sentiment']+1\n","merge_data['Sentiment']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_FQ3NGZG_q1","outputId":"8b08a558-4263-45c1-c9ed-da8edb57a662","execution":{"iopub.status.busy":"2023-10-17T22:54:18.794797Z","iopub.execute_input":"2023-10-17T22:54:18.795102Z","iopub.status.idle":"2023-10-17T22:54:18.80341Z","shell.execute_reply.started":"2023-10-17T22:54:18.79508Z","shell.execute_reply":"2023-10-17T22:54:18.802665Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603251366,"user_tz":-420,"elapsed":21,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       0.0\n","1       2.0\n","2       1.0\n","3       2.0\n","4       0.0\n","       ... \n","2994    1.0\n","2995    2.0\n","2996    2.0\n","2997    2.0\n","2998    1.0\n","Name: Sentiment, Length: 2999, dtype: float64"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["merge_data['Sentences'] = pd.Series([uts.word_tokenize(sent ,format=\"text\") for sent in merge_data['Sentences']])"],"metadata":{"execution":{"iopub.status.busy":"2023-10-17T22:54:18.804499Z","iopub.execute_input":"2023-10-17T22:54:18.804788Z","iopub.status.idle":"2023-10-17T22:54:32.827351Z","shell.execute_reply.started":"2023-10-17T22:54:18.804758Z","shell.execute_reply":"2023-10-17T22:54:32.826624Z"},"trusted":true,"id":"WFwxTU2D0Nv4","executionInfo":{"status":"ok","timestamp":1697603275922,"user_tz":-420,"elapsed":24572,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"q4qAHotCcUe5","execution":{"iopub.status.busy":"2023-10-17T22:54:32.828473Z","iopub.execute_input":"2023-10-17T22:54:32.828702Z","iopub.status.idle":"2023-10-17T22:54:32.832213Z","shell.execute_reply.started":"2023-10-17T22:54:32.828683Z","shell.execute_reply":"2023-10-17T22:54:32.831453Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603275922,"user_tz":-420,"elapsed":21,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_train, X_val, y_train, y_val = train_test_split(merge_data['Sentences'], merge_data['Sentiment'],\n","                                                                    random_state=2023,\n","                                                                    test_size=0.3)"],"metadata":{"id":"xTpSXM7scmdN","execution":{"iopub.status.busy":"2023-10-17T22:54:32.833363Z","iopub.execute_input":"2023-10-17T22:54:32.833646Z","iopub.status.idle":"2023-10-17T22:54:32.846882Z","shell.execute_reply.started":"2023-10-17T22:54:32.833625Z","shell.execute_reply":"2023-10-17T22:54:32.846214Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603275923,"user_tz":-420,"elapsed":21,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base-v2')"],"metadata":{"id":"6RjRL6aIc7S9","colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["b11e0e4056a347c88c52243624c29b5e","29e7247cf9da42ffa09939e0937be4f6","1ba99172f5574d27a8bedffdfbd466e5","ebd4c7ed379a46fbb1360bc81bb77294","d47fe59d1a0040b4ab12f6a4d1952c15","6cc29c5099ab4c83ab5c7866d961457c","9525b8c275d24b3e9ca724a47137721d","9b5701ff1c8f48a385d7db9c57474227","6757b19d9e844f6a8e5b8d0f8dc10f3b","eaeac0ee2c0c42c4904e061eb24312fc","8ad528247d4d4ecfb47065dd999d0bc7","555025445a034c44958ddb447a527543","33d329ccb9ba430aae10c4dd0880f483","724178cd49c54edd81c1a02f86a4f5fa","b966d36c776d41e581d612e020e26b15","356c37aa7da54a758e2569fbdc2777b8","4b980379fdb94d8785c0600272f50bba","0cbf6a2f28c74c3e848377bbed0f4109","80540b63342f4bab974f5fcdd786f0eb","dd552cb9c9a749518b547439bf680f1b","c4074b06fec946909a65780bf165605c","9acf94dca7f64b298dd120fd2b82a723","e8ef79fd3c504b8280a388a5c1dd1095","fd1e3876decd4975b09e0331524d184e","bd7f69cdb6894c16bd90f65b4949a439","6360ac2a395d45d08e70320f80766c80","0601ec4b9677414d94de381db5f5ce55","b63cfae0c72d425a93a0b114f436ef0a","7c0217fc772d4fb69b2452669f82c964","5863d629b8794756bc640eb24774c721","ddd5ecbd6f4b40179ed2b6313ba9baec","e68ba1554113468e8a4b22a7a9c381f9","e4999b05dc66400ebd999058210e8bde","0e898f3657c443ffadac7bc12e233ff8","9584375023814562babef20caf662c69","5d456240a9344b99b82dcf4d4173c680","da77928ac4464fcd95476f09d6452a3b","35a71eec038f4f01a5c5c810cc534c32","c4663a30669d41c9aaee021b1acfb7eb","a678c53bd3a649c3bdaf3f36f4d6edb4","3cfb3821863c4008af8732d25d6dc946","7202f164dbca459089e8999e2ef9eadb","54bce615b90945d2aa6d77634be90d73","1a420e50ed6b467183cd746f048ea7b5"]},"outputId":"7ce9bb97-0ab3-4e6e-b687-c5f905b2fa06","execution":{"iopub.status.busy":"2023-10-17T22:54:32.848105Z","iopub.execute_input":"2023-10-17T22:54:32.848748Z","iopub.status.idle":"2023-10-17T22:54:33.145618Z","shell.execute_reply.started":"2023-10-17T22:54:32.848724Z","shell.execute_reply":"2023-10-17T22:54:33.144925Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603277984,"user_tz":-420,"elapsed":2081,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b11e0e4056a347c88c52243624c29b5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"555025445a034c44958ddb447a527543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8ef79fd3c504b8280a388a5c1dd1095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e898f3657c443ffadac7bc12e233ff8"}},"metadata":{}}]},{"cell_type":"code","source":["def preprocessing_for_bert(data):\n","    \"\"\"Perform required preprocessing steps for pretrained BERT.\n","    @param    data (np.array): Array of texts to be processed.\n","    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n","    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n","                  tokens should be attended to by the model.\n","    \"\"\"\n","    # Create empty lists to store outputs\n","    input_ids = []\n","    attention_masks = []\n","\n","    for sent in data:\n","        # `encode_plus` will:\n","        #    (1) Tokenize the sentence\n","        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n","        #    (3) Truncate/Pad sentence to max length\n","        #    (4) Map tokens to their IDs\n","        #    (5) Create attention mask\n","        #    (6) Return a dictionary of outputs\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,\n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=256,                  # Max length to truncate/pad\n","            padding='max_length',         # Pad sentence to max length\n","            return_attention_mask=True,\n","            truncation= True,\n","            )\n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","    # Convert lists to tensors\n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","\n","    return input_ids, attention_masks"],"metadata":{"id":"ZpFo6SzS_Hsi","execution":{"iopub.status.busy":"2023-10-17T22:54:33.146836Z","iopub.execute_input":"2023-10-17T22:54:33.147131Z","iopub.status.idle":"2023-10-17T22:54:33.154053Z","shell.execute_reply.started":"2023-10-17T22:54:33.14709Z","shell.execute_reply":"2023-10-17T22:54:33.153226Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603277985,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["print('Tokenizing data...')\n","train_inputs, train_masks = preprocessing_for_bert(X_train)\n","val_inputs, val_masks = preprocessing_for_bert(X_val)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XpgqAXwEZSN","outputId":"c866c7a6-3f9f-4694-e55c-066a71d7dc2f","execution":{"iopub.status.busy":"2023-10-17T22:54:33.15656Z","iopub.execute_input":"2023-10-17T22:54:33.156813Z","iopub.status.idle":"2023-10-17T22:54:34.877855Z","shell.execute_reply.started":"2023-10-17T22:54:33.156784Z","shell.execute_reply":"2023-10-17T22:54:34.876724Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603283826,"user_tz":-420,"elapsed":5856,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing data...\n"]}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Convert other data types to torch.Tensor\n","train_labels = torch.tensor(y_train.values).type(torch.LongTensor)\n","val_labels = torch.tensor(y_val.values).type(torch.LongTensor)\n","\n","# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n","batch_size = 32\n","\n","# Create the DataLoader for our training set\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","# Create the DataLoader for our validation set\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_sampler = SequentialSampler(val_data)\n","val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"],"metadata":{"id":"_e-2CnONEHby","execution":{"iopub.status.busy":"2023-10-17T22:54:34.879748Z","iopub.execute_input":"2023-10-17T22:54:34.880194Z","iopub.status.idle":"2023-10-17T22:54:34.888894Z","shell.execute_reply.started":"2023-10-17T22:54:34.880166Z","shell.execute_reply":"2023-10-17T22:54:34.88774Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603283827,"user_tz":-420,"elapsed":20,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["class BertClassifier(nn.Module):\n","    \"\"\"Bert Model for Classification Tasks.\n","    \"\"\"\n","    def __init__(self, freeze_bert=False):\n","        \"\"\"\n","        @param    bert: a BertModel object\n","        @param    classifier: a torch.nn.Module classifier\n","        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n","        \"\"\"\n","        super(BertClassifier, self).__init__()\n","        num_classes = 3\n","\n","        # Instantiate BERT model\n","        self.bert = AutoModel.from_pretrained('vinai/phobert-base-v2')\n","\n","        # Instantiate an one-layer feed-forward classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(768, 50),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(50, num_classes)\n","        )\n","\n","        # Freeze the BERT model\n","        if freeze_bert:\n","            for param in self.bert.parameters():\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        \"\"\"\n","        Feed input to BERT and the classifier to compute logits.\n","        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n","                      max_length)\n","        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n","                      information with shape (batch_size, max_length)\n","        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n","                      num_labels)\n","        \"\"\"\n","        # Feed input to BERT\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        # Extract the last hidden state of the token `[CLS]` for classification task\n","        last_hidden_state_cls = outputs[0][:, 0, :]\n","        logits = self.classifier(last_hidden_state_cls)\n","\n","        return logits"],"metadata":{"id":"bqS2BvagUm3F","execution":{"iopub.status.busy":"2023-10-17T22:54:34.890531Z","iopub.execute_input":"2023-10-17T22:54:34.890828Z","iopub.status.idle":"2023-10-17T22:54:34.901276Z","shell.execute_reply.started":"2023-10-17T22:54:34.890797Z","shell.execute_reply":"2023-10-17T22:54:34.900541Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603283827,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","def initialize_model(epochs=4):\n","    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n","    \"\"\"\n","    # Instantiate Bert Classifier\n","    bert_classifier = BertClassifier(freeze_bert=False)\n","    bert_classifier.to(device)\n","    # Create the optimizer\n","    optimizer = AdamW(bert_classifier.parameters(),\n","                      lr=5e-5,    # Default learning rate\n","                      eps=1e-8    # Default epsilon value\n","                      )\n","    # Total number of training steps\n","    total_steps = len(train_dataloader) * epochs\n","    # Set up the learning rate scheduler\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps=0, # Default value\n","                                                num_training_steps=total_steps)\n","    return bert_classifier, optimizer, scheduler"],"metadata":{"id":"eBPMemCc92zc","execution":{"iopub.status.busy":"2023-10-17T22:54:34.902254Z","iopub.execute_input":"2023-10-17T22:54:34.9027Z","iopub.status.idle":"2023-10-17T22:54:34.915327Z","shell.execute_reply.started":"2023-10-17T22:54:34.902663Z","shell.execute_reply":"2023-10-17T22:54:34.914663Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603283827,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import random\n","import time\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\n","    \"\"\"\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n","    \"\"\"Train the BertClassifier model.\n","    \"\"\"\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    for epoch_i in range(epochs):\n","        # Print the header of the result table\n","        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n","        print(\"-\"*70)\n","        # Measure the elapsed time of each epoch\n","        t0_epoch, t0_batch = time.time(), time.time()\n","        # Reset tracking variables at the beginning of each epoch\n","        total_loss, batch_loss, batch_counts = 0, 0, 0\n","        # Put the model into the training mode\n","        model.train()\n","        model = nn.DataParallel(model)\n","        # For each batch of training data...\n","        for step, batch in enumerate(train_dataloader):\n","            batch_counts +=1\n","            # Load batch to GPU\n","            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids, b_attn_mask)\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            batch_loss += loss.item()\n","            total_loss += loss.item()\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            # Update parameters and the learning rate\n","            optimizer.step()\n","            scheduler.step()\n","            # Print the loss values and time elapsed for every 20 batches\n","            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n","                # Calculate time elapsed for 20 batches\n","                time_elapsed = time.time() - t0_batch\n","                # Print training results\n","                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n","                # Reset batch tracking variables\n","                batch_loss, batch_counts = 0, 0\n","                t0_batch = time.time()\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","        print(\"-\"*70)\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if evaluation == True:\n","            # After the completion of each training epoch, measure the model's performance\n","            # on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            print(\"-\"*70)\n","        print(\"\\n\")\n","    print(\"Training complete!\")"],"metadata":{"id":"FUJbiCQAUm0m","execution":{"iopub.status.busy":"2023-10-17T22:54:34.916181Z","iopub.execute_input":"2023-10-17T22:54:34.916576Z","iopub.status.idle":"2023-10-17T22:54:34.934596Z","shell.execute_reply.started":"2023-10-17T22:54:34.916554Z","shell.execute_reply":"2023-10-17T22:54:34.933534Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697603283828,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"metadata":{"execution":{"iopub.status.busy":"2023-10-17T22:54:34.93581Z","iopub.execute_input":"2023-10-17T22:54:34.93623Z","iopub.status.idle":"2023-10-17T22:54:34.949013Z","shell.execute_reply.started":"2023-10-17T22:54:34.9362Z","shell.execute_reply":"2023-10-17T22:54:34.948194Z"},"trusted":true,"id":"4HvqeqwX0Nv-","executionInfo":{"status":"ok","timestamp":1697603283828,"user_tz":-420,"elapsed":19,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","def evaluate_multiclass_roc(probs, y_true, num_classes):\n","    \"\"\"\n","    - Print AUC and accuracy for each class in a multiclass classification problem\n","    - Plot ROC for each class\n","    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), num_classes)\n","    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n","    @params    num_classes (int): number of classes in the classification problem\n","    \"\"\"\n","    # Initialize lists to store AUC and accuracy values for each class\n","    auc_values = []\n","    accuracy_values = []\n","\n","    # Plot ROC for each class\n","    plt.figure(figsize=(8, 6))\n","\n","    for i in range(num_classes):\n","        class_probs = probs[:, i]\n","        fpr, tpr, _ = roc_curve(y_true, class_probs, pos_label=i)\n","        roc_auc = auc(fpr, tpr)\n","        auc_values.append(roc_auc)\n","\n","        # Get accuracy for the current class\n","        class_predictions = (class_probs >= 0.5).astype(int)\n","        class_accuracy = accuracy_score(y_true, class_predictions)\n","        accuracy_values.append(class_accuracy)\n","\n","        # Plot ROC curve for the current class\n","        plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n","\n","    # Plot random guessing line\n","    plt.plot([0, 1], [0, 1], 'k--')\n","\n","    # Set plot labels and title\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","    # Print AUC and accuracy for each class\n","    for i in range(num_classes):\n","        print(f'Class {i} - AUC: {auc_values[i]:.4f}, Accuracy: {accuracy_values[i] * 100:.2f}%')\n","\n","# Example usage:\n","# probs: predicted probabilities with shape (len(y_true), num_classes)\n","# y_true: true labels with shape (len(y_true),)\n","# num_classes: number of classes in the classification problem\n","# evaluate_multiclass_roc(probs, y_true, num_classes)"],"metadata":{"execution":{"iopub.status.busy":"2023-10-17T22:54:34.950442Z","iopub.execute_input":"2023-10-17T22:54:34.950818Z","iopub.status.idle":"2023-10-17T22:54:34.965247Z","shell.execute_reply.started":"2023-10-17T22:54:34.95079Z","shell.execute_reply":"2023-10-17T22:54:34.96433Z"},"trusted":true,"id":"Umi6TIUf0Nv_","executionInfo":{"status":"ok","timestamp":1697603283828,"user_tz":-420,"elapsed":18,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def bert_predict(model, test_dataloader):\n","    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n","    on the test set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    all_logits = []\n","\n","    # For each batch in our test set...\n","    for batch in test_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids, b_attn_mask)\n","        all_logits.append(logits)\n","\n","    # Concatenate logits from each batch\n","    all_logits = torch.cat(all_logits, dim=0)\n","\n","    # Apply softmax to calculate probabilities\n","    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n","\n","    return probs"],"metadata":{"execution":{"iopub.status.busy":"2023-10-17T22:54:34.966582Z","iopub.execute_input":"2023-10-17T22:54:34.967275Z","iopub.status.idle":"2023-10-17T22:54:34.978946Z","shell.execute_reply.started":"2023-10-17T22:54:34.967231Z","shell.execute_reply":"2023-10-17T22:54:34.977828Z"},"trusted":true,"id":"Pn2BNyIg0NwA","executionInfo":{"status":"ok","timestamp":1697603284288,"user_tz":-420,"elapsed":478,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["set_seed(42)    # Set seed for reproducibility\n","num_epochs = 20\n","bert_classifier, optimizer, scheduler = initialize_model(epochs=num_epochs)\n","train(bert_classifier, train_dataloader, val_dataloader, epochs=num_epochs, evaluation=True)"],"metadata":{"id":"LqtMUYFc5MST","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6c4c01c438c24bd4a6c8eb5a9b478713","08f72e1461fb417c86ff6fff0917e121","09bc91778be446f1ba6a2ce35fc611ac","4742cdad96514fafb7d469e4bfd67625","6c2a6c2f76cb4252b99a3e9bab285f85","8dde6ea7e8a04e2b90f8e35fd4a11e61","27f92c78758743ab8357ddabfb830243","70bac8180dc74d3d86766067c882d9e6","089b3c011d1e46fc80cc1d0c9aa7898b","bb64ecd0056045efa1f0eaf0b241c987","3fde2f01c593450f8f7fe448ea198e64"]},"outputId":"3178cdff-a075-440a-b31b-e49518bb0a06","execution":{"iopub.status.busy":"2023-10-17T22:54:34.980059Z","iopub.execute_input":"2023-10-17T22:54:34.980365Z"},"trusted":true,"executionInfo":{"status":"ok","timestamp":1697605221958,"user_tz":-420,"elapsed":1937673,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}}},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4c01c438c24bd4a6c8eb5a9b478713"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Start training...\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   1    |   20    |   1.076872   |     -      |     -     |   27.73  \n","   1    |   40    |   1.098122   |     -      |     -     |   24.01  \n","   1    |   60    |   1.045992   |     -      |     -     |   24.48  \n","   1    |   65    |   1.015805   |     -      |     -     |   5.74   \n","----------------------------------------------------------------------\n","   1    |    -    |   1.069327   |  1.018601  |   48.06   |   93.79  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   2    |   20    |   1.010410   |     -      |     -     |   26.78  \n","   2    |   40    |   0.978108   |     -      |     -     |   25.45  \n","   2    |   60    |   0.950312   |     -      |     -     |   25.23  \n","   2    |   65    |   0.950216   |     -      |     -     |   5.85   \n","----------------------------------------------------------------------\n","   2    |    -    |   0.977850   |  0.975921  |   56.25   |   95.29  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   3    |   20    |   0.905268   |     -      |     -     |   26.72  \n","   3    |   40    |   0.898808   |     -      |     -     |   25.45  \n","   3    |   60    |   0.888470   |     -      |     -     |   25.44  \n","   3    |   65    |   0.947472   |     -      |     -     |   5.85   \n","----------------------------------------------------------------------\n","   3    |    -    |   0.901418   |  0.958121  |   54.74   |   95.47  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   4    |   20    |   0.798422   |     -      |     -     |   26.77  \n","   4    |   40    |   0.790038   |     -      |     -     |   25.55  \n","   4    |   60    |   0.813182   |     -      |     -     |   25.49  \n","   4    |   65    |   0.804353   |     -      |     -     |   5.86   \n","----------------------------------------------------------------------\n","   4    |    -    |   0.800804   |  0.938516  |   58.51   |   95.72  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   5    |   20    |   0.701513   |     -      |     -     |   26.70  \n","   5    |   40    |   0.749076   |     -      |     -     |   25.50  \n","   5    |   60    |   0.768430   |     -      |     -     |   25.49  \n","   5    |   65    |   0.608681   |     -      |     -     |   5.89   \n","----------------------------------------------------------------------\n","   5    |    -    |   0.729171   |  0.946045  |   59.27   |   95.66  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   6    |   20    |   0.684658   |     -      |     -     |   26.76  \n","   6    |   40    |   0.637996   |     -      |     -     |   25.50  \n","   6    |   60    |   0.599852   |     -      |     -     |   25.54  \n","   6    |   65    |   0.592278   |     -      |     -     |   5.91   \n","----------------------------------------------------------------------\n","   6    |    -    |   0.637821   |  0.942442  |   60.99   |   95.82  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   7    |   20    |   0.557072   |     -      |     -     |   26.83  \n","   7    |   40    |   0.552282   |     -      |     -     |   25.56  \n","   7    |   60    |   0.529512   |     -      |     -     |   25.51  \n","   7    |   65    |   0.562396   |     -      |     -     |   5.90   \n","----------------------------------------------------------------------\n","   7    |    -    |   0.547672   |  0.955913  |   60.02   |   95.93  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   8    |   20    |   0.494886   |     -      |     -     |   26.81  \n","   8    |   40    |   0.491083   |     -      |     -     |   25.56  \n","   8    |   60    |   0.446860   |     -      |     -     |   25.59  \n","   8    |   65    |   0.521133   |     -      |     -     |   5.94   \n","----------------------------------------------------------------------\n","   8    |    -    |   0.481168   |  1.057938  |   58.84   |   96.11  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","   9    |   20    |   0.466999   |     -      |     -     |   26.86  \n","   9    |   40    |   0.432991   |     -      |     -     |   25.58  \n","   9    |   60    |   0.393954   |     -      |     -     |   25.57  \n","   9    |   65    |   0.397740   |     -      |     -     |   5.90   \n","----------------------------------------------------------------------\n","   9    |    -    |   0.429312   |  1.045970  |   59.48   |   96.09  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  10    |   20    |   0.365909   |     -      |     -     |   26.88  \n","  10    |   40    |   0.355340   |     -      |     -     |   25.65  \n","  10    |   60    |   0.334116   |     -      |     -     |   25.60  \n","  10    |   65    |   0.516327   |     -      |     -     |   5.92   \n","----------------------------------------------------------------------\n","  10    |    -    |   0.364468   |  1.169232  |   60.78   |   96.25  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  11    |   20    |   0.301987   |     -      |     -     |   26.98  \n","  11    |   40    |   0.365141   |     -      |     -     |   25.67  \n","  11    |   60    |   0.370886   |     -      |     -     |   25.59  \n","  11    |   65    |   0.320585   |     -      |     -     |   5.93   \n","----------------------------------------------------------------------\n","  11    |    -    |   0.343412   |  1.289448  |   56.68   |   96.40  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  12    |   20    |   0.263847   |     -      |     -     |   26.87  \n","  12    |   40    |   0.291824   |     -      |     -     |   25.66  \n","  12    |   60    |   0.358650   |     -      |     -     |   25.63  \n","  12    |   65    |   0.202715   |     -      |     -     |   5.95   \n","----------------------------------------------------------------------\n","  12    |    -    |   0.296422   |  1.282023  |   58.84   |   96.37  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  13    |   20    |   0.267660   |     -      |     -     |   26.96  \n","  13    |   40    |   0.279402   |     -      |     -     |   25.60  \n","  13    |   60    |   0.239393   |     -      |     -     |   25.67  \n","  13    |   65    |   0.225001   |     -      |     -     |   5.95   \n","----------------------------------------------------------------------\n","  13    |    -    |   0.259421   |  1.332607  |   59.27   |   96.46  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  14    |   20    |   0.238862   |     -      |     -     |   26.94  \n","  14    |   40    |   0.252639   |     -      |     -     |   25.70  \n","  14    |   60    |   0.239130   |     -      |     -     |   25.77  \n","  14    |   65    |   0.273437   |     -      |     -     |   5.95   \n","----------------------------------------------------------------------\n","  14    |    -    |   0.245737   |  1.404404  |   58.30   |   96.69  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  15    |   20    |   0.222743   |     -      |     -     |   27.03  \n","  15    |   40    |   0.218940   |     -      |     -     |   25.72  \n","  15    |   60    |   0.225950   |     -      |     -     |   25.72  \n","  15    |   65    |   0.231735   |     -      |     -     |   5.96   \n","----------------------------------------------------------------------\n","  15    |    -    |   0.223244   |  1.496895  |   57.76   |   96.81  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  16    |   20    |   0.182579   |     -      |     -     |   27.06  \n","  16    |   40    |   0.205891   |     -      |     -     |   25.74  \n","  16    |   60    |   0.224530   |     -      |     -     |   25.74  \n","  16    |   65    |   0.225938   |     -      |     -     |   5.95   \n","----------------------------------------------------------------------\n","  16    |    -    |   0.205641   |  1.426402  |   60.88   |   96.88  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  17    |   20    |   0.167239   |     -      |     -     |   27.14  \n","  17    |   40    |   0.172331   |     -      |     -     |   25.87  \n","  17    |   60    |   0.229141   |     -      |     -     |   25.86  \n","  17    |   65    |   0.233345   |     -      |     -     |   5.97   \n","----------------------------------------------------------------------\n","  17    |    -    |   0.192548   |  1.512735  |   59.27   |   97.27  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  18    |   20    |   0.208728   |     -      |     -     |   27.21  \n","  18    |   40    |   0.163764   |     -      |     -     |   25.96  \n","  18    |   60    |   0.191867   |     -      |     -     |   25.93  \n","  18    |   65    |   0.127954   |     -      |     -     |   5.94   \n","----------------------------------------------------------------------\n","  18    |    -    |   0.183874   |  1.547063  |   59.05   |   97.50  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  19    |   20    |   0.165555   |     -      |     -     |   27.17  \n","  19    |   40    |   0.192047   |     -      |     -     |   25.90  \n","  19    |   60    |   0.162686   |     -      |     -     |   25.85  \n","  19    |   65    |   0.170788   |     -      |     -     |   5.98   \n","----------------------------------------------------------------------\n","  19    |    -    |   0.173110   |  1.519597  |   60.02   |   97.39  \n","----------------------------------------------------------------------\n","\n","\n"," Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","----------------------------------------------------------------------\n","  20    |   20    |   0.185679   |     -      |     -     |   27.25  \n","  20    |   40    |   0.155649   |     -      |     -     |   25.97  \n","  20    |   60    |   0.139247   |     -      |     -     |   25.99  \n","  20    |   65    |   0.252802   |     -      |     -     |   6.02   \n","----------------------------------------------------------------------\n","  20    |    -    |   0.167594   |  1.523586  |   59.48   |   97.77  \n","----------------------------------------------------------------------\n","\n","\n","Training complete!\n"]}]},{"cell_type":"code","source":["# # Compute predicted probabilities on the test set\n","# probs = bert_predict(bert_classifier, val_dataloader)\n","\n","# # Evaluate the Bert classifier\n","# evaluate_multiclass_roc(probs, y_val, 3)"],"metadata":{"trusted":true,"id":"A-58wrop0NwB","executionInfo":{"status":"error","timestamp":1697605520104,"user_tz":-420,"elapsed":12204,"user":{"displayName":"Pham Hai Dang","userId":"14329947320801718745"}},"colab":{"base_uri":"https://localhost:8080/","height":348},"outputId":"54b07597-82c4-44b0-838a-c8b8d3e343d3"},"execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-07344095bbbe>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate the Bert classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mevaluate_multiclass_roc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-a01f754c061e>\u001b[0m in \u001b[0;36mevaluate_multiclass_roc\u001b[0;34m(probs, y_true, num_classes)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Get accuracy for the current class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mclass_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclass_probs\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mclass_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0maccuracy_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x600 with 0 Axes>"]},"metadata":{}}]}]}